## Лабораторная работа 2
Выполнил:  
Якушев Алексей 

## Примечание
В среде Lunar Lander были проблемы в генерации шума, поэтому в данной лобораторной работе используется среда BipedalWalker
Также гитхаб не поддерживает файлы размерности больше 50Мб поэтому строки с визуализацией были удалены

## Описание

В контексте обучения с подкреплением (Reinforcement Learning, RL), стратегии обучения модели определяют, каким образом агент исследует окружение и выбирает действия для максимизации награды. Эти стратегии разделяются на Exploration (исследование) и Exploitation (использование) и направлены на достижение баланса между изучением новых стратегий и максимизацией текущих знаний. В обучении с подкреплением присутствует баланс между Exploration и Exploitation. Exploration включает в себя стратегии, направленные на изучение новых действий или состояний, чтобы расширить базу знаний. Exploitation, наоборот, использует текущие знания для выбора оптимальных действий и максимизации награды. Нахождение оптимального баланса между Exploration и Exploitation - ключевой аспект в достижении успешных стратегий обучения с подкреплением. В данной работе мы рассмотрим различные стратегии и их влияние на обучение агента с использованием библиотек Gymnasium и Stable-Baseline3.

## Ход работы
В данной работе были рассмотрены две стратегии DDPG и SAC, а также исследовано влияние learning_rate на значение награды

Первым этапом было рассмотрена стратегия DDPG, которая использует epsilon-greedy стратегию, добавляя шум к выбранному действию во время исследования
В рамках работы были рассмотрены сценарии при различных значениях коэффициента sigma отвечающиего за зашумленность
Значение total_timesteps = 150 000

Вторым этапом была рассмотрена стратегия softmax черезе термин энтропии SAC.
В данном сценарии менось значение learning_rate 
Было рассмотрено три сценария при learnign_rate равном 0.001, 0.01, 0.1

Согласно полученной анимации значением learning_rate равное 0.001 показалось наиболее оптимальным

Последним этапом было сравнение моделей с разным значением learning_rate и был построен сравнительный график
В данном эксперименте сравнивались значения learning_rate равные 0.01, 0.001, 0.0001
Наибольшая награда была у модели с значением learning_rate = 0.001