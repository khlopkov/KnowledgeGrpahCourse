{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-21T23:05:41.207517200Z",
          "start_time": "2024-01-21T23:05:38.488161Z"
        },
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "103c16c01e1e0494",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-21T23:05:41.209518100Z",
          "start_time": "2024-01-21T23:05:41.207517200Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "103c16c01e1e0494"
      },
      "outputs": [],
      "source": [
        "def q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
        "  # Доступ к Q-network\n",
        "  q_net = model.q_net\n",
        "\n",
        "  # Конвертируем observation в PyTorch tensor\n",
        "  obs_tensor = th.tensor(obs, dtype=th.float32)\n",
        "\n",
        "  # Изменяем размерность\n",
        "  obs_tensor = obs_tensor.unsqueeze(0)\n",
        "\n",
        "  #Извлекаем Q-values\n",
        "  q_values = model.q_net.forward(obs_tensor.cuda())\n",
        "\n",
        "  return q_values.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea894d8c407a41d",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-01-21T23:05:41.208517700Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "dea894d8c407a41d"
      },
      "outputs": [],
      "source": [
        "def plot_q_values(q_values_list):\n",
        "\n",
        "  # Построение графика\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  for i in range(6):\n",
        "    plt.plot(q_values_list[i], label='Q%i Values' % i)\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Q-Values')\n",
        "  plt.title('Convergence of Q-Values over Time')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1271ddb8400489c1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-21T23:05:41.209518100Z",
          "start_time": "2024-01-21T23:05:41.209518100Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "1271ddb8400489c1"
      },
      "outputs": [],
      "source": [
        "def mean_reward(discount_factor):\n",
        "  #Создание окружения\n",
        "  env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "\n",
        "  #Создание модели\n",
        "  model = DQN(\"MlpPolicy\", env, verbose=1, gamma=discount_factor, learning_rate=0.001)\n",
        "\n",
        "  #Количество эпизодов для оценки модели\n",
        "  n_eval_episodes = 250\n",
        "\n",
        "  #Оценка модели до обучения\n",
        "  mean_reward, std_reward = evaluate_policy(model, gym.make(\"Taxi-v3\", render_mode=\"rgb_array\"), deterministic=True, n_eval_episodes=n_eval_episodes)\n",
        "  print(f\"До обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "  # Засекаем начальное время\n",
        "  start_time = time.time()\n",
        "\n",
        "  #Обучение модели\n",
        "  model.learn(total_timesteps=10000, log_interval=100)\n",
        "\n",
        "  # Засекаем время завершения\n",
        "  end_time = time.time()\n",
        "\n",
        "  #Оценка модели после обучения\n",
        "  mean_reward, std_reward = evaluate_policy(model, gym.make(\"Taxi-v3\", render_mode=\"rgb_array\"), deterministic=True, n_eval_episodes=n_eval_episodes)\n",
        "  print(f\"После обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "  model.save(f\"dqn_taxi_{discount_factor}\")\n",
        "  del model\n",
        "\n",
        "  # Вычисляем время обучений\n",
        "  learn_time = end_time - start_time\n",
        "  return env, learn_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae86020442b78ed5",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-01-21T23:05:41.210517800Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ae86020442b78ed5"
      },
      "outputs": [],
      "source": [
        "def q_values_calculation(discount_factor, env):\n",
        "  #Загружаем созданную и созраненную модель\n",
        "  model = DQN.load(f\"dqn_taxi_{discount_factor}\")\n",
        "\n",
        "  action_str = ['down', 'up', 'right', 'left', \"pick up\", \"drop off\"]\n",
        "  q_values_list = []\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  for _ in range(100):\n",
        "      q_val = q_values(model,obs)\n",
        "      q_values_list.append(q_val)\n",
        "      action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "      print(f\"Q-value состояния down={q_val[0]:.2f} up={q_val[1]:.2f} right={q_val[2]:.2f} left={q_val[3]:.2f} pick up={q_val[4]:.2f} drop off={q_val[5]:.2f}\")\n",
        "      print(f\"Действие: {action_str[action]}\")\n",
        "\n",
        "      obs, reward, terminated, truncated, _ = env.step(int(action))\n",
        "\n",
        "  return q_values_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8030b16163aed64",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-01-21T23:05:41.212021800Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "b8030b16163aed64"
      },
      "outputs": [],
      "source": [
        "discount_factors = [0.01,0.2,0.7,0.9]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f191e3d2c31247",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-21T23:05:41.214025900Z",
          "start_time": "2024-01-21T23:05:41.213025200Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "84f191e3d2c31247"
      },
      "outputs": [],
      "source": [
        "for discount_factor in discount_factors:\n",
        "  environment, time_to_lrn = mean_reward(discount_factor)\n",
        "  q_vals = q_values_calculation(discount_factor, environment)\n",
        "  plot_q_values(q_vals)\n",
        "  print(f'Время обучения модели при discount_factor = {discount_factor} : {time_to_lrn} секунд.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f2ea4a0fdb8515",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2024-01-20T22:01:21.215594300Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "64f2ea4a0fdb8515"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813f0145afbaa4ea",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "813f0145afbaa4ea"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}