### Эксперименты с Обучением С Подкреплением - README
Выполнили:  
Веснин Дмитрий  
Евгений Федотовских    
#### Описание
Цель этих экспериментов - подобрать оптимальный коэффициент обучения (Learning Rate, lr) для стратегии обучения с подкреплением. Мы провели тестирование Soft Actor-Critic (SAC) и DDPG с различными значениеми коэффициента обучения (0.0001, 0.001, 0.01, 0.0004, 0.004, 0.04) с использованием стратегий и softmax b Epsilon-Greedy соответсвенно.
Выбрана среда MountainCarContinuous-v0


#### Результаты Экспериментов
Первоначальный эксперимент с SAC

При коэффициенте обучения 0.02
Средняя награда: -1.739-06
Не удалось достичь сходимость, отсутсвие колебаний. 

При коэффициенте обучения 0.002
Средняя награда: -4.4611e-07
Этот коэффициент обучения вызвал колебания в процессе обучения и долгую сходимость.

При коэффициенте обучения 0.0002
Средняя награда: -1.739e-06
Этот коэффициент обучения вызвал колебания в процессе обучения и долгую сходимость. 

<img src="lr1.png" alt="drawing" width="400"/>

После первоначального эксперимента SAC

Следующие эксперименты проводились со значениями lr: 0.0003, 0.005, 0.00005

При коэффициенте обучения 0.0003
Средняя награда: -4.809e-05
Сходимость не была достигнута даже после 500000 шагов.

При коэффициенте обучения 0.005
Средняя награда: -3.45-e06
Сходимость также не была достигнута даже после 500000 шагов.

При коэффициенте обучения 0.00005
Средняя награда: -2.81-e06
Сходимость также не была достигнута даже после 500000 шагов.

<img src="lr2.png" alt="drawing" width="400"/>


#### Заключение
Эти эксперименты показывают, что для поиска правильного коэффициента обучения для данной среды требуется много проб и ошибок. Возможно данная стратегия обучения не совсем хорошо подходит для данной среды (MountainCarContinuous-v0)