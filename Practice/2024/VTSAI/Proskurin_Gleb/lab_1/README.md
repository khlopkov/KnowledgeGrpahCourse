# Лабораторная работа 1

Выполнили:
* Герасимчук Михаил (P4241)
* Проскурин Глеб (P4241)

### Reinforcement Learning (RL)

Reinforcement Learning (Обучение с подкреплением) — это класс машинного обучения, в котором агент обучается принимать решения путем взаимодействия с окружающей средой. Агент принимает действия, получает обратную связь в виде вознаграждения или штрафа, и стремится максимизировать кумулятивное вознаграждение.

### Q-learning
[Q-learning](https://en.wikipedia.org/wiki/Q-learning) — это один из методов обучения с подкреплением, используемый для обучения агента принимать оптимальные действия в конкретной среде. Агент стремится выучить функцию Q, которая оценивает ожидаемую награду для каждой пары состояние-действие.

### Тестирование и Валидация

В контексте RL, тестирование и валидация играют важную роль. После обучения модели агента необходимо оценить ее производительность (тестирование) и убедиться в ее способности обобщения к различным ситуациям (валидация).

Тестирование в RL включает в себя запуск обученной модели в реальной среде и измерение ее производительности на основе определенных метрик. Валидация, с другой стороны, может включать в себя проверку способности модели адаптироваться к новым условиям или изменениям в среде.

## Выбор среды

В данной работе мы будем использовать среду [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/#frozen-lake), предоставляемую Gym. 

## Параметры среды
 * `map_name="4x4`: Среда представляет собой сетку 4x4, где агент начинает с одного угла, а цель находится в противоположном углу.
* `is_slippery=False`: Поверхность нескользкая, что обеспечивает детерминированное поведение агента. В отличие от стандартного поведения, когда агент может скользить в случайном направлении, здесь он движется точно так, как задано.

А также среду [ALE/Pong-v5](https://www.gymlibrary.dev/environments/atari/pong/)
Вы управляете правым веслом и соревнуетесь с левым веслом, управляемым компьютером. Каждый из вас пытается отклонить мяч от своих ворот в ворота соперника. Подробную документацию можно найти на странице AtariAge.

# Действия
0 НООП
1 ОГОНЬ
2 ВЕРНО
3 ЛЕВЫЙ
4 ПРАВЫЙ ОГОНЬ
5 ЛЕВЫЙ ОГОНЬ

## Параметры обучения
 * `total_timesteps=1000000`: Общее количество временных шагов, в течение которых будет проходить обучение модели. Это значение указывает на глубину и продолжительность обучения, что важно для достижения оптимальных результатов.
* `log_interval=100000`: Интервал логирования установлен на каждые 100000 шагов. Это позволяет отслеживать прогресс обучения модели, получая отчеты о её производительности через равные промежутки времени.

## Выводы
1. Изучили библиотки для создания среды и обучения моделей с подкреплением.
2. После обучения агент успешно достиг нужной ячейки.
