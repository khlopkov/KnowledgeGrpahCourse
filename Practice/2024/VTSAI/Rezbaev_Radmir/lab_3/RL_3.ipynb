{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dc68cd5"
      },
      "source": [
        "## Discount factor\n",
        "**Discount factor**  - это параметр, используемый в теории управления и обучении с подкреплением для оценки стоимости будущих вознаграждений. Он обозначается символом $\\gamma$ (гамма).\n",
        "\n",
        "В контексте задачи обучения с подкреплением, такой как у **DQN** алгоритма, фактор дисконтирования применяется к будущим вознаграждениям, чтобы учесть уменьшение их значения с течением времени. Фактор дисконтирования позволяет агенту принимать во внимание не только мгновенные вознаграждения, но и будущие вознаграждения, приводя к более долгосрочной стратегии.\n",
        "\n",
        "**Формула для дисконтирования будущих вознаграждений** выглядит следующим образом:\n",
        "\n",
        " $G_t$ = $R_{t+1}$ + $\\gamma$ $R_{t+2}$ + $\\gamma$^2 $R_{t+3}$ + $\\ldots$ = $\\sum_{k=0}^{\\infty}$ $\\gamma$^k $R_{t+k+1}$\n",
        "\n",
        "где:\n",
        "- $G_t$ - дисконтированная сумма вознаграждений (вознаграждение с учетом будущих шагов);\n",
        "- $R_{t+k+1}$ - вознаграждение, полученное на шаге $t+k+1$;\n",
        "- $\\gamma$ - фактор дисконтирования,  $0 \\leq \\gamma \\leq 1$.\n",
        "\n",
        "Фактор дисконтирования имеет важное значение при принятии решений в условиях неопределенности, где агенту нужно учитывать как мгновенные, так и будущие вознаграждения, с учетом их временного удаления. Выбор подходящего значения для фактора дисконтирования может влиять на стратегии обучения.\n"
      ],
      "id": "8dc68cd5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-term и Short-term стратегия\n",
        "\n",
        "Фактор дисконтирования оказывает сильное влияние на долгосрочность стратегии.\n",
        "В контексте обучения с подкреплением и стратегий агента, термины \"краткосрочные\" и \"долгосрочные\" относятся к временным характеристикам принятия решений агентом.\n",
        "\n",
        "**Краткосрочные стратегии**:\n",
        "\n",
        "> Агент, придерживающийся краткосрочной стратегии, ориентируется в основном на текущую информацию и мгновенные вознаграждения.\n",
        "Такой агент может принимать решения, которые приносят максимальное мгновенное вознаграждение, не уделяя большого внимания будущим шагам. Такой агент может принимать решения, которые приносят максимальное мгновенное вознаграждение, не уделяя большого внимания будущим шагам.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Долгосрочные стратегии**:\n",
        "\n",
        "\n",
        "> Агент, придерживающийся долгосрочной стратегии, учитывает будущие вознаграждения и последствия своих действий на протяжении более длительного времени.\n",
        "Такой агент может предпочесть действия, которые могут не приносить максимальное мгновенное вознаграждение, но могут способствовать достижению более высоких наград в будущем.\n",
        "\n",
        "Когда говорят о краткосрочных или долгосрочных стратегиях в контексте обучения с подкреплением, обычно имеется в виду, как агент принимает решения, оптимизируя сумму будущих вознаграждений. Фактор дисконтирования γ в уравнении обучения с подкреплением регулирует степень учета будущих вознаграждений, и, таким образом, влияет на то, насколько агент ориентирован на краткосрочные или долгосрочные перспективы."
      ],
      "metadata": {
        "id": "pgaFe0w08Gg2"
      },
      "id": "pgaFe0w08Gg2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метрики для оценки модели\n",
        "В данной работе мы рассмотрим некоторые метрики, которые могут быть использованы для оценки обучения модели.\n",
        "Аналогично первой лабораторной работы мы будем решать задачу в окружении CartPole-v1, а в качестве модели используем уже знакомый DQN. При этом мы обучим модель несколько разных с разным параметром Discount factor.\n",
        "\n",
        "Для оценки модели мы рассмотрим следущие метрики:\n",
        "\n",
        "*   Q-values, их поведение и сходимость.\n",
        "*   Средняя награда за эпизод.\n",
        "*   Время затрачиваемое на обучение модели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZIN2MIo1pJ5"
      },
      "id": "4ZIN2MIo1pJ5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221acc63"
      },
      "source": [
        "### Установка зависимостей\n",
        "\n",
        "На первом шаге мы начинаем с установки необходимых библиотек. Пакет [gymnasium](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) предоставляет различные среды для обучения с подкреплением, в то время как [stable-baselines3](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) предоставляет реализации различных алгоритмов обучения с подкреплением."
      ],
      "id": "221acc63"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc3a1b2f",
        "scrolled": true,
        "outputId": "e4be7e73-9328-4a90-e016-ac71186afd0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 7,816 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.7 [28.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.7 [865 kB]\n",
            "Fetched 7,816 kB in 1s (7,125 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 121671 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.7_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.7_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting gymnasium[accept-rom-license,atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2023.11.17)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=8bbff8100f55a94d8855bd671bdb892ce595ab30be8a5db7524eed754cfb366b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.2.1\n",
            "Collecting PyVirtualDisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: PyVirtualDisplay\n",
            "Successfully installed PyVirtualDisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "# Установка необходимых библиотек\n",
        "!sudo apt install xvfb\n",
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install stable-baselines3\n",
        "!pip install PyVirtualDisplay"
      ],
      "id": "fc3a1b2f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем пакет, которые будут использованы в данной работе\n"
      ],
      "metadata": {
        "id": "Z1Fbb4br3TK2"
      },
      "id": "Z1Fbb4br3TK2"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import time"
      ],
      "metadata": {
        "id": "IWD3A_wi5qHh"
      },
      "id": "IWD3A_wi5qHh",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-значения (Q-values) представляют собой оценки или оценочные функции, используемые в контексте обучения с подкреплением для измерения ожидаемой полезности (ценности) принятия определенного действия в конкретном состоянии.\n",
        "\n",
        "Для получения значений для нашей модели реализуем функцию, которая их извлекает из q-сети."
      ],
      "metadata": {
        "id": "ctnQDvYs3Xpe"
      },
      "id": "ctnQDvYs3Xpe"
    },
    {
      "cell_type": "code",
      "source": [
        "def q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
        "  # Доступ к Q-network\n",
        "  q_net = model.q_net\n",
        "\n",
        "  # Конвертируем observation в PyTorch tensor\n",
        "  obs_tensor = th.tensor(obs, dtype=th.float32)\n",
        "\n",
        "  # Изменяем размерность\n",
        "  obs_tensor = obs_tensor.unsqueeze(0)\n",
        "\n",
        "  #Извлекаем Q-values\n",
        "  q_values = model.q_net.forward(obs_tensor)\n",
        "\n",
        "  return q_values.detach().numpy()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdVK2QGXDcH3",
        "outputId": "dd096535-b51a-402f-b4f5-0f2cb4f1d505"
      },
      "id": "JdVK2QGXDcH3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для наглядности результатов Q-values необходимо создать функцию для составления графика по значениям."
      ],
      "metadata": {
        "id": "BDriZ7ML4X2H"
      },
      "id": "BDriZ7ML4X2H"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_q_values(q_values_list):\n",
        "  # Извлечение отдельных списков для каждого Q-значения\n",
        "  q1_values, q2_values, q3_values, q4_values = zip(*q_values_list)\n",
        "\n",
        "  # Построение графика\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(q1_values, label='Q1 Values')\n",
        "  plt.plot(q2_values, label='Q2 Values')\n",
        "  plt.plot(q3_values, label='Q3 Values')\n",
        "  plt.plot(q4_values, label='Q4 Values')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Q-Values')\n",
        "  plt.title('Convergence of Q-Values over Time')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "f0ZIyURWpg8h"
      },
      "id": "f0ZIyURWpg8h",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий блок кода служит для следующих целей:\n",
        "\n",
        "*   Создание окружения\n",
        "*   Создание модели\n",
        "*   Обучение модели\n",
        "*   Оценка модели до обучения\n",
        "*   Оценка модели после обучения\n",
        "*   Измерение времени обучени\n"
      ],
      "metadata": {
        "id": "kdwvvV754rHo"
      },
      "id": "kdwvvV754rHo"
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_reward(discount_factor):\n",
        "  #Создание окружения\n",
        "  env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "  #Создание модели\n",
        "  model = DQN(\"MlpPolicy\", env, verbose=1, gamma=discount_factor, learning_rate=0.001, device='cuda')\n",
        "\n",
        "  #Количество эпизодов для оценки модели\n",
        "  n_eval_episodes = 250\n",
        "\n",
        "  #Оценка модели до обучения\n",
        "  mean_reward, std_reward = evaluate_policy(model, gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\"), deterministic=True, n_eval_episodes=n_eval_episodes)\n",
        "  print(f\"До обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "  # Засекаем начальное время\n",
        "  start_time = time.time()\n",
        "\n",
        "  #Обучение модели\n",
        "  model.learn(total_timesteps=10000, log_interval=100)\n",
        "\n",
        "  # Засекаем время завершения\n",
        "  end_time = time.time()\n",
        "\n",
        "  #Оценка модели после обучения\n",
        "  mean_reward, std_reward = evaluate_policy(model, gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\"), deterministic=True, n_eval_episodes=n_eval_episodes)\n",
        "  print(f\"После обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "  model.save(f\"dqn_mountaint_car_{discount_factor}\")\n",
        "  del model\n",
        "\n",
        "  # Вычисляем время обучений\n",
        "  learn_time = end_time - start_time\n",
        "  return env, learn_time"
      ],
      "metadata": {
        "id": "uOdMyNh7QKER"
      },
      "id": "uOdMyNh7QKER",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-values необходимо вычислять для некоторого набора предсказаний модели, реализуем для этого функцию"
      ],
      "metadata": {
        "id": "9keB1T9Y519e"
      },
      "id": "9keB1T9Y519e"
    },
    {
      "cell_type": "code",
      "source": [
        "def q_values_calculation(discount_factor, env):\n",
        "  #Загружаем созданную и созраненную модель\n",
        "  model = DQN.load(f\"dqn_mountain_car_{discount_factor}\")\n",
        "\n",
        "  action_str = ['down','up','right','left','pick up','drop off']\n",
        "  q_values_list = []\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  for _ in range(100):\n",
        "      q_val = q_values(model,obs)\n",
        "      q_values_list.append(q_val)\n",
        "      action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "      print(f\"Q-value состояния down={q_val[0]:.2f} up={q_val[1]:.2f} right={q_val[2]:.2f} left={q_val[3]:.2f} pick up={q_val[4]:.2f} drop off={q_val[5]:.2f}\")\n",
        "      print(f\"Действие: {action_str[action]}\")\n",
        "\n",
        "      obs, reward, terminated, truncated, info = env.step(int(action))\n",
        "\n",
        "  return q_values_list"
      ],
      "metadata": {
        "id": "G3-n6IC_cxs3"
      },
      "id": "G3-n6IC_cxs3",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изменение значения discount factor (γ) в DQN может оказать существенное воздействие на обучение агента и его стратегию.\n",
        "\n",
        "Воздействие на краткосрочные или долгосрочные стратегии:\n",
        "\n",
        "*   При более высоком значении γ (ближе к 1) агент будет больше учитывать будущие вознаграждения, что может способствовать формированию более долгосрочных стратегий.\n",
        "\n",
        "*  При более низком значении γ (ближе к 0) агент будет более склонен к учету только мгновенных вознаграждений, что может привести к формированию краткосрочных стратегий.\n",
        "\n",
        "\n",
        "Влияние на стабильность обучения:\n",
        "\n",
        "*   Высокий γ может сделать обучение более стабильным, особенно в средах с большим количеством шума или случайности.\n",
        "*   Низкий γ может привести к более нестабильному обучению, так как агент будет менее чувствителен к долгосрочным последствиям своих действий.\n",
        "\n",
        "\n",
        "Влияние на временные рамки обучения:\n",
        "\n",
        "\n",
        "*   Различные значения γ могут потребовать различных временных рамок обучения для достижения оптимальных стратегий.\n",
        "*   Изменение γ может потребовать перенастройки гиперпараметров и количества шагов обучения.\n",
        "\n",
        "\n",
        "Адаптивность к задаче:\n",
        "*  Эффективное значение γ может зависеть от конкретной задачи. Например, в задачах с большой неопределенностью и случайностью, более высокий γ может быть полезен.\n",
        "\n",
        "\n",
        "Экспериментальные результаты:\n",
        "*  Чтобы определить оптимальное значение γ для вашей конкретной задачи, часто требуется провести эксперименты с различными значениями, а затем анализировать результаты."
      ],
      "metadata": {
        "id": "j37Byubd6fEI"
      },
      "id": "j37Byubd6fEI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим 3 значения discount factor"
      ],
      "metadata": {
        "id": "utVz8HWu7pXd"
      },
      "id": "utVz8HWu7pXd"
    },
    {
      "cell_type": "code",
      "source": [
        "discount_factors = [0.01,0.5,0.99]"
      ],
      "metadata": {
        "id": "Me6HrWLOvPSV"
      },
      "id": "Me6HrWLOvPSV",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for discount_factor in discount_factors:\n",
        "  environment, time_to_lrn = mean_reward(discount_factor)\n",
        "  q_vals = q_values_calculation(discount_factor, environment)\n",
        "  plot_q_values(q_vals)\n",
        "  print(f'Время обучения модели при discount_factor = {discount_factor} : {time_to_lrn} секунд.')"
      ],
      "metadata": {
        "id": "f7bljIvKmNVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d251290-12b3-4bb2-8794-17d24b7dc4ae"
      },
      "id": "f7bljIvKmNVY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c178e6"
      },
      "source": [
        "#### Закрытие среды после тестирования\n",
        "По завершении тестирования мы закрываем среду с помощью env.close()."
      ],
      "id": "57c178e6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "912717a1"
      },
      "outputs": [],
      "source": [
        "# Закрытие среды\n",
        "environment.close()"
      ],
      "id": "912717a1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa11d65b"
      },
      "source": [
        "В рамках самостоятельной работы попробуйте обучить и протестировать модель на другой среды доступной в gymnasium."
      ],
      "id": "aa11d65b"
    },
    {
      "cell_type": "code",
      "source": [
        "environment, time_to_lrn = mean_reward(discount_factors[1])\n",
        "q_vals = q_values_calculation(discount_factors[1], environment)\n",
        "plot_q_values(q_vals)\n",
        "print(f'Время обучения модели при discount_factor = {discount_factors[1]} : {time_to_lrn} секунд.')"
      ],
      "metadata": {
        "id": "uGqXV8xwuuYG"
      },
      "id": "uGqXV8xwuuYG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Закрытие среды\n",
        "environment.close()"
      ],
      "metadata": {
        "id": "0iZn9Uywy666"
      },
      "id": "0iZn9Uywy666",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment, time_to_lrn = mean_reward(discount_factors[2])\n",
        "q_vals = q_values_calculation(discount_factors[2], environment)\n",
        "plot_q_values(q_vals)\n",
        "print(f'Время обучения модели при discount_factor = {discount_factors[2]} : {time_to_lrn} секунд.')"
      ],
      "metadata": {
        "id": "XeqFpyzbyvOl"
      },
      "id": "XeqFpyzbyvOl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Закрытие среды\n",
        "environment.close()"
      ],
      "metadata": {
        "id": "6uQqaFsiy9ai"
      },
      "id": "6uQqaFsiy9ai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы завершили лабораторную работу, посвященную изучению дисконтного фактора и ряда метрик, связанных с обучением с подкреплением на примере алгоритма DQN.\n",
        "\n",
        "Для того чтобы закрепить полученные знания, предлагается подобрать наиболее оптимальный параметр для рассмторенной задачи или самостоятельно реализовать решение другой задачи, настраивая дисконтный фактор и анализируя изученные метрики. Это позволит лучше понять влияние дисконтного фактора на обучение агента в различных контекстах и освоить навыки тюнинга параметров для достижения оптимальных результатов.\n"
      ],
      "metadata": {
        "id": "DRO6Jtru73mo"
      },
      "id": "DRO6Jtru73mo"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}