# Лабораторная работа 2

Выполнили:
* Герасимчук Михаил (P4241)
* Проскурин Глеб (P4241)

В рамках данной лабораторной работы мы провели серию экспериментов и провели тестирование различных стратегий, таких как:
* `DDPG`, который использует epsilon-greedy стратегию, добавляя шум к выбранному действию во время исследования.
* `SAC`.Стратегия исследования softmax применяется через термин энтропии в SAC. Термин энтропия поощряет мягкое, вероятностное распределение действий, способствуя исследованию.

Провели исследование влияния Learning Rate:
* `Learning Rate (LR)` является одним из ключевых гиперпараметров алгоритмов обучения с подкреплением, включая алгоритм Soft Actor-Critic (SAC). Этот параметр оказывает значительное влияние на эффективность обучения и способность алгоритма адаптироваться к изменениям в окружающей среде


## Выводы
1. Провели анализ алгоримов `SAC`и `DDPG`. Алгоритм `DDPG` стоит использовать, когда мы работаем с непрерывными пространствами действий, а `SAC` помогает бороться с проблемами излишнего доверия к поведению агента.

2. Несмотря на малоуспешные результаты в "CarRacing-v2", можно проследить следующие закономерности относительно learning rate к-та:

    * Маленький learning rate: Обучение будет медленным, так как обновления весов модели будут небольшими. Может потребоваться больше времени для достижения сходимости. Существует риск застревания в локальных минимумах.

    * Средний learning rate: Может оказаться хорошим компромиссом между скоростью обучения и стабильностью. Быстрее, чем маленький learning_rate, но может все еще требовать достаточно большое количество времени для сходимости. Требует аккуратного подбора значения.

    * Большой learning rate: Обучение в среднем будет быстрее, так как веса модели обновляются с большими шагами. Может привести к нестабильному обучению, особенно если learning rate слишком велик. Существует риск "перепрыгивания" оптимальных значений, что может замедлить или прервать сходимость.

Эвристики, которые были вскольз упомянуты на первом курсе - значения LR в диапазоне [0.002, 0.005] - оказались оптимальными.

В данной лабораторной работе мы исследовали различные стратегии и влияние `learning_rate` на процесс обучения агента. Эксперименты позволяют нам понять, что различные стратегии и параметры лучше подходят для разных задач.

Разница в наградах в ходе обучения между `epsilon-greedy` стратегией и `softmax` минимальна. Единственное отличие - softmax имеет большую дисперсию, что свидетельствует о большем приоритете в исследовании.