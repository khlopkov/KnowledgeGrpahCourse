{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dc68cd5",
      "metadata": {
        "id": "8dc68cd5"
      },
      "source": [
        "## Discount factor\n",
        "**Discount factor**  - это параметр, используемый в теории управления и обучении с подкреплением для оценки стоимости будущих вознаграждений. Он обозначается символом $\\gamma$ (гамма).\n",
        "\n",
        "В контексте задачи обучения с подкреплением, такой как у **DQN** алгоритма, фактор дисконтирования применяется к будущим вознаграждениям, чтобы учесть уменьшение их значения с течением времени. Фактор дисконтирования позволяет агенту принимать во внимание не только мгновенные вознаграждения, но и будущие вознаграждения, приводя к более долгосрочной стратегии.\n",
        "\n",
        "**Формула для дисконтирования будущих вознаграждений** выглядит следующим образом:\n",
        "\n",
        " $G_t$ = $R_{t+1}$ + $\\gamma$ $R_{t+2}$ + $\\gamma$^2 $R_{t+3}$ + $\\ldots$ = $\\sum_{k=0}^{\\infty}$ $\\gamma$^k $R_{t+k+1}$\n",
        "\n",
        "где:\n",
        "- $G_t$ - дисконтированная сумма вознаграждений (вознаграждение с учетом будущих шагов);\n",
        "- $R_{t+k+1}$ - вознаграждение, полученное на шаге $t+k+1$;\n",
        "- $\\gamma$ - фактор дисконтирования,  $0 \\leq \\gamma \\leq 1$.\n",
        "\n",
        "Фактор дисконтирования имеет важное значение при принятии решений в условиях неопределенности, где агенту нужно учитывать как мгновенные, так и будущие вознаграждения, с учетом их временного удаления. Выбор подходящего значения для фактора дисконтирования может влиять на стратегии обучения.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pgaFe0w08Gg2",
      "metadata": {
        "id": "pgaFe0w08Gg2"
      },
      "source": [
        "## Long-term и Short-term стратегия\n",
        "\n",
        "Фактор дисконтирования оказывает сильное влияние на долгосрочность стратегии.\n",
        "В контексте обучения с подкреплением и стратегий агента, термины \"краткосрочные\" и \"долгосрочные\" относятся к временным характеристикам принятия решений агентом.\n",
        "\n",
        "**Краткосрочные стратегии**:\n",
        "\n",
        "> Агент, придерживающийся краткосрочной стратегии, ориентируется в основном на текущую информацию и мгновенные вознаграждения.\n",
        "Такой агент может принимать решения, которые приносят максимальное мгновенное вознаграждение, не уделяя большого внимания будущим шагам. Такой агент может принимать решения, которые приносят максимальное мгновенное вознаграждение, не уделяя большого внимания будущим шагам.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Долгосрочные стратегии**:\n",
        "\n",
        "\n",
        "> Агент, придерживающийся долгосрочной стратегии, учитывает будущие вознаграждения и последствия своих действий на протяжении более длительного времени.\n",
        "Такой агент может предпочесть действия, которые могут не приносить максимальное мгновенное вознаграждение, но могут способствовать достижению более высоких наград в будущем.\n",
        "\n",
        "Когда говорят о краткосрочных или долгосрочных стратегиях в контексте обучения с подкреплением, обычно имеется в виду, как агент принимает решения, оптимизируя сумму будущих вознаграждений. Фактор дисконтирования γ в уравнении обучения с подкреплением регулирует степень учета будущих вознаграждений, и, таким образом, влияет на то, насколько агент ориентирован на краткосрочные или долгосрочные перспективы."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ZIN2MIo1pJ5",
      "metadata": {
        "id": "4ZIN2MIo1pJ5"
      },
      "source": [
        "## Метрики для оценки модели\n",
        "В данной работе мы рассмотрим некоторые метрики, которые могут быть использованы для оценки обучения модели.\n",
        "Мы будем решать задачу в окружении Taxi-v3, а в качестве модели используем уже знакомый DQN. При этом мы обучим модель несколько разных с разным параметром Discount factor.\n",
        "\n",
        "Для оценки модели мы рассмотрим следущие метрики:\n",
        "\n",
        "*   Q-values, их поведение и сходимость.\n",
        "*   Средняя награда за эпизод.\n",
        "*   Время затрачиваемое на обучение модели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221acc63",
      "metadata": {
        "id": "221acc63"
      },
      "source": [
        "### Установка зависимостей\n",
        "\n",
        "На первом шаге мы начинаем с установки необходимых библиотек. Пакет [gymnasium](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) предоставляет различные среды для обучения с подкреплением, в то время как [stable-baselines3](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) предоставляет реализации различных алгоритмов обучения с подкреплением."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fc3a1b2f",
      "metadata": {
        "id": "fc3a1b2f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70013dd4-8a25-460d-9146-c6e968b05804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 7,816 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.7 [28.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.7 [865 kB]\n",
            "Fetched 7,816 kB in 1s (6,723 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 121671 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.7_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.7_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.2.1\n",
            "Collecting PyVirtualDisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: PyVirtualDisplay\n",
            "Successfully installed PyVirtualDisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install xvfb\n",
        "!pip install gymnasium\n",
        "!pip install stable-baselines3\n",
        "!pip install PyVirtualDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z1Fbb4br3TK2",
      "metadata": {
        "id": "Z1Fbb4br3TK2"
      },
      "source": [
        "Импортируем пакет, которые будут использованы в данной работе\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "IWD3A_wi5qHh",
      "metadata": {
        "id": "IWD3A_wi5qHh"
      },
      "outputs": [],
      "source": [
        "# Импорт библиотек\n",
        "import torch as th\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ctnQDvYs3Xpe",
      "metadata": {
        "id": "ctnQDvYs3Xpe"
      },
      "source": [
        "Q-значения (Q-values) представляют собой оценки или оценочные функции, используемые в контексте обучения с подкреплением для измерения ожидаемой полезности (ценности) принятия определенного действия в конкретном состоянии.\n",
        "\n",
        "Для получения значений для нашей модели реализуем функцию, которая их извлекает из q-сети."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ab162642",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab162642",
        "outputId": "76cf43a0-17c9-4bee-f7fa-50657a19aaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "ENV_NAME = \"MountainCar-v0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "JdVK2QGXDcH3",
      "metadata": {
        "id": "JdVK2QGXDcH3"
      },
      "outputs": [],
      "source": [
        "def q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
        "  q_net = model.q_net\n",
        "  obs_tensor = th.tensor(obs, dtype=th.float32)\n",
        "  obs_tensor = obs_tensor.unsqueeze(0)\n",
        "  q_values = model.q_net.forward(obs_tensor)\n",
        "  return q_values.detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BDriZ7ML4X2H",
      "metadata": {
        "id": "BDriZ7ML4X2H"
      },
      "source": [
        "Для наглядности результатов Q-values необходимо создать функцию для составления графика по значениям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f0ZIyURWpg8h",
      "metadata": {
        "id": "f0ZIyURWpg8h"
      },
      "outputs": [],
      "source": [
        "def plot_q_values(q_values_list):\n",
        "  q1_values, q2_values, q3_values = zip(*q_values_list)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(q1_values, label='Q1 Values')\n",
        "  plt.plot(q2_values, label='Q2 Values')\n",
        "  plt.plot(q3_values, label='Q3 Values')\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel('Q-Values')\n",
        "  plt.title('Convergence of Q-Values over Time')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kdwvvV754rHo",
      "metadata": {
        "id": "kdwvvV754rHo"
      },
      "source": [
        "Следующий блок кода служит для следующих целей:\n",
        "\n",
        "*   Создание окружения\n",
        "*   Создание модели\n",
        "*   Обучение модели\n",
        "*   Оценка модели до обучения\n",
        "*   Оценка модели после обучения\n",
        "*   Измерение времени обучени\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "uOdMyNh7QKER",
      "metadata": {
        "id": "uOdMyNh7QKER"
      },
      "outputs": [],
      "source": [
        "def mean_reward(discount_factor):\n",
        "  env = gym.make(ENV_NAME, render_mode=\"human\")\n",
        "  model = DQN(\"MlpPolicy\", env, verbose=1, gamma=discount_factor)\n",
        "  n_eval_episodes = 250\n",
        "\n",
        "  mean_reward, std_reward = evaluate_policy(model, gym.make(ENV_NAME, render_mode=\"human\"), deterministic=True, n_eval_episodes=n_eval_episodes)\n",
        "  print(f\"До обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  model.learn(total_timesteps=10000, log_interval=100)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  mean_reward, std_reward = evaluate_policy(model, gym.make(ENV_NAME, render_mode=\"human\"), deterministic=True, n_eval_episodes=n_eval_episodes)\n",
        "  print(f\"После обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "  model.save(f\"dqn_cartpole_{discount_factor}\")\n",
        "  del model\n",
        "\n",
        "  learn_time = end_time - start_time\n",
        "  return env, learn_time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9keB1T9Y519e",
      "metadata": {
        "id": "9keB1T9Y519e"
      },
      "source": [
        "Q-values необходимо вычислять для некоторого набора предсказаний модели, реализуем для этого функцию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "G3-n6IC_cxs3",
      "metadata": {
        "id": "G3-n6IC_cxs3"
      },
      "outputs": [],
      "source": [
        "def q_values_calculation(discount_factor, env):\n",
        "  model = DQN.load(f\"dqn_cartpole_{discount_factor}\")\n",
        "\n",
        "  action_str = [\"left\",\"no\", \"right\"]\n",
        "  q_values_list = []\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  for _ in range(100):\n",
        "      q_val = q_values(model,obs)\n",
        "      q_values_list.append(q_val)\n",
        "      action, _states = model.predict(obs, deterministic=True)\n",
        "\n",
        "      print(f\"Q-value состояния left={q_val[0]:.2f} no={q_val[1]:.2f} right={q_val[2]:.2f}\")\n",
        "      print(f\"Действие: {action_str[action]}\")\n",
        "\n",
        "      obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  return q_values_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j37Byubd6fEI",
      "metadata": {
        "id": "j37Byubd6fEI"
      },
      "source": [
        "Изменение значения discount factor (γ) в DQN может оказать существенное воздействие на обучение агента и его стратегию.\n",
        "\n",
        "Воздействие на краткосрочные или долгосрочные стратегии:\n",
        "\n",
        "*   При более высоком значении γ (ближе к 1) агент будет больше учитывать будущие вознаграждения, что может способствовать формированию более долгосрочных стратегий.\n",
        "\n",
        "*  При более низком значении γ (ближе к 0) агент будет более склонен к учету только мгновенных вознаграждений, что может привести к формированию краткосрочных стратегий.\n",
        "\n",
        "\n",
        "Влияние на стабильность обучения:\n",
        "\n",
        "*   Высокий γ может сделать обучение более стабильным, особенно в средах с большим количеством шума или случайности.\n",
        "*   Низкий γ может привести к более нестабильному обучению, так как агент будет менее чувствителен к долгосрочным последствиям своих действий.\n",
        "\n",
        "\n",
        "Влияние на временные рамки обучения:\n",
        "\n",
        "\n",
        "*   Различные значения γ могут потребовать различных временных рамок обучения для достижения оптимальных стратегий.\n",
        "*   Изменение γ может потребовать перенастройки гиперпараметров и количества шагов обучения.\n",
        "\n",
        "\n",
        "Адаптивность к задаче:\n",
        "*  Эффективное значение γ может зависеть от конкретной задачи. Например, в задачах с большой неопределенностью и случайностью, более высокий γ может быть полезен.\n",
        "\n",
        "\n",
        "Экспериментальные результаты:\n",
        "*  Чтобы определить оптимальное значение γ для вашей конкретной задачи, часто требуется провести эксперименты с различными значениями, а затем анализировать результаты."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utVz8HWu7pXd",
      "metadata": {
        "id": "utVz8HWu7pXd"
      },
      "source": [
        "Рассмотрим 3 значения discount factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Me6HrWLOvPSV",
      "metadata": {
        "id": "Me6HrWLOvPSV"
      },
      "outputs": [],
      "source": [
        "discount_factors = [0.01,0.5,0.99]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7bljIvKmNVY",
      "metadata": {
        "id": "f7bljIvKmNVY"
      },
      "outputs": [],
      "source": [
        "environment, time_to_lrn = mean_reward(discount_factors[0])\n",
        "q_vals = q_values_calculation(discount_factors[0], environment)\n",
        "print(f'Время обучения модели при discount_factor = {discount_factors[0]} : {time_to_lrn} секунд.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_q_values(q_vals)"
      ],
      "metadata": {
        "id": "xkNzOdjVOo5s"
      },
      "id": "xkNzOdjVOo5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "57c178e6",
      "metadata": {
        "id": "57c178e6"
      },
      "source": [
        "#### Закрытие среды после тестирования\n",
        "По завершении тестирования мы закрываем среду с помощью env.close()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912717a1",
      "metadata": {
        "id": "912717a1"
      },
      "outputs": [],
      "source": [
        "# Закрытие среды\n",
        "environment.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uGqXV8xwuuYG",
      "metadata": {
        "id": "uGqXV8xwuuYG"
      },
      "outputs": [],
      "source": [
        "environment, time_to_lrn = mean_reward(discount_factors[1])\n",
        "q_vals = q_values_calculation(discount_factors[1], environment)\n",
        "print(f'Время обучения модели при discount_factor = {discount_factors[1]} : {time_to_lrn} секунд.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_q_values(q_vals)"
      ],
      "metadata": {
        "id": "Ru6Ce8h1OqXs"
      },
      "id": "Ru6Ce8h1OqXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0iZn9Uywy666",
      "metadata": {
        "id": "0iZn9Uywy666"
      },
      "outputs": [],
      "source": [
        "environment.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XeqFpyzbyvOl",
      "metadata": {
        "id": "XeqFpyzbyvOl"
      },
      "outputs": [],
      "source": [
        "environment, time_to_lrn = mean_reward(discount_factors[2])\n",
        "q_vals = q_values_calculation(discount_factors[2], environment)\n",
        "print(f'Время обучения модели при discount_factor = {discount_factors[2]} : {time_to_lrn} секунд.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_q_values(q_vals)"
      ],
      "metadata": {
        "id": "GpjlfDhoOrgF"
      },
      "id": "GpjlfDhoOrgF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6uQqaFsiy9ai",
      "metadata": {
        "id": "6uQqaFsiy9ai"
      },
      "outputs": [],
      "source": [
        "environment.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}