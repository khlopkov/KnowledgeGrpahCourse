## Лабораторная работа 2

Хаяров Э.А. P4241

### Задание

1. Изучите ноутбук для данной работы
2. Выберите две стратегии для своей среды
3. Провести эксперименты, изменяя параметры стратегий, 
чтобы более глубоко понять их влияние на обучение 
модели.
4. Опишите выводы в readme и загрузите свои файлы на 
github.

### Выполнение
[Ссылка на код](./RL_№2.ipynb)

### Вывод
В этой лабораторной работе, помимо изучения стратегий разведки и эксплуатации, мы также провели сравнительный анализ двух алгоритмов обучения с подкреплением: мягких критических факторов (SAC) и глубоких детерминированных градиентов политики (DDPG).

Алгоритм SAC - это метод обучения с подкреплением, который активно использует стратегии исследований и использования. Он основан на принципе максимизации энтропии, который помогает бороться с проблемой чрезмерного доверия к действиям агента. В ходе экспериментов, проведенных в лаборатории, мы оценили, как SAC справляется с балансом между исследованиями и использованием и какими результатами он достигает в выбранной задаче.

В свою очередь, DDPG - это алгоритм, специально разработанный для задач с непрерывными пространствами действий. Он также активно использует стратегии разведки и эксплуатации, и на его производительность может сильно повлиять выбор гиперпараметров, таких как скорость обучения. В лаборатории мы сравнили производительность DDPG с производительностью SAC, а также изучили, как изменение скорости обучения влияет на обучение по обоим алгоритмам.

Этот сравнительный анализ помог студентам лучше понять, как различные алгоритмы ведут себя в средах разведки и эксплуатации. Это также позволило нам оценить, как различные стратегии и параметры влияют на процесс обучения и результаты каждого алгоритма.