# Лабораторная работа 2

Выполнил Гораш Вячеслав, гр. P4240

### Описание окружения Bipedal Walker

В лабораторной работе номер 1 использовалась среда Taxi. Однако алгоритмы, предлагаемые к исследованию во второй работе,
не умеют работать с дискретным пространством действий. Поэтому среда была изменена. 

Новая среда имеет 4 непрерывных пространства действий (по одному на сустав). Задача агента - пройти максимально
возможное состояние, чтобы корпус не касался земли.

Награда составляет +300 за продвижение вперед, -100 за падение

### Обучение с помощью эпсилон-жадной стратегии

В качестве модели использовалась DDPG с параметром sigma равным 0.1, 0.05 и 0.3. При значениях 0.1 и 0.05 модель сходилась
на стратегии, когда агент не продвигается вперед, но при этом не падает. Скорее всего, это связано с тем, что даже при
выборе произвольного действия (исследовательская стратегия) вероятность совпадения действий, при которых возможно
продвижение вперед, крайне мала. При этом довольно быстро формируется стратегия без риска. Этому также способствует то,
что штраф за нахождение без продвижения гораздо меньше, чем за падение. Это побуждает модель не рисковать. При значении
0.3 модель выбирает слишком рискованные варианты и сразу падает.

### Обучение с помощью softmax
Использовались 4 различные значения learning rate

- 1e-5: застревание в локальном минимуме. Модель научилась сохранять положение устойчивого равновесия без продвижения вперед
- 1e-4: застревание в локальном минимуме. В отличие от предыдущего случая есть незначительное продвижение вперед, однако большинство действий направлены на удержание равновесия
- 1e-3: успешное обучение. Модель смогла пройти достаточно большое расстояние без падений. Оптимальное значение для алгоритма в данных условиях.
- 1e-2: расхождение. Модель не обучилась. Делает слишком резкие движения и сразу падает.

### Вывод
Среди исследуемых вариантов наилучший результат показала SAC модель с learning rate 1e-3. Остальные значения параметра
приводят либо к застреванию в локальном минимуме либо к расхождению. Модель на основе эпсилон-жадной стратегии
не смогла обучиться полностью, либо останавливаясь в локальном минимуме, либо ведя себя слишком рискованно. Возможно, для данного
алгоритма требуется более тонкая настройка параметров.