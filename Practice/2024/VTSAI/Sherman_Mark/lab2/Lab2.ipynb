{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc68cd5",
   "metadata": {
    "id": "8dc68cd5"
   },
   "source": [
    "## Стратегии обучения RL\n",
    "\n",
    "В контексте обучения с подкреплением (Reinforcement Learning, RL), стратегии обучения модели определяют, каким образом агент исследует окружение и выбирает действия для максимизации награды. Эти стратегии разделяются на Exploration (исследование) и Exploitation (использование) и направлены на достижение баланса между изучением новых стратегий и максимизацией текущих знаний.\n",
    "В обучении с подкреплением присутствует баланс между Exploration и Exploitation. Exploration включает в себя стратегии, направленные на изучение новых действий или состояний, чтобы расширить базу знаний. Exploitation, наоборот, использует текущие знания для выбора оптимальных действий и максимизации награды. Нахождение оптимального баланса между Exploration и Exploitation - ключевой аспект в достижении успешных стратегий обучения с подкреплением.\n",
    "В данной работе мы рассмотрим различные стратегии и их влияние на обучение агента с использованием библиотек Gymnasium и Stable-Baseline3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221acc63",
   "metadata": {
    "id": "221acc63"
   },
   "source": [
    "### Подготовка среды и библиотек\n",
    "\n",
    "Установим необходимые библиотеки и создадим среду CartPole для экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a1b2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc3a1b2f",
    "outputId": "4521d151-9c0b-4377-872f-ebc5abe3f5a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install stable-baselines3\n",
    "!pip install PyVirtualDisplay\n",
    "!sudo apt-get install xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc7a12",
   "metadata": {
    "id": "afcc7a12"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC,DDPG\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b153d15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b153d15",
    "outputId": "c63cd6fa-486b-4925-dfb4-7d7e7df3c2ba"
   },
   "outputs": [],
   "source": [
    "# Создание окружения MountainCarContinuous-v0\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2713d",
   "metadata": {
    "id": "c8e2713d"
   },
   "source": [
    "### Исследование различных стратегий\n",
    "Исследуем влияние различных стратегий исследования на процесс обучения агента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677e9b6",
   "metadata": {
    "id": "8677e9b6"
   },
   "source": [
    "#### Epsilon-Greedy\n",
    "\n",
    "Epsilon-Greedy - это одна из базовых стратегий исследования в обучении с подкреплением. Агент принимает решение о выборе действия с учетом двух возможных вариантов: с высокой вероятностью (1-epsilon) агент выбирает действие с максимальной оценкой (использование), а с низкой вероятностью (epsilon) - случайное действие (исследование). Это позволяет агенту совмещать использование текущих знаний с возможностью исследования новых стратегий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d2bbc",
   "metadata": {
    "id": "f74d2bbc"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(Q_values))  # Исследование\n",
    "    else:\n",
    "        return np.argmax(Q_values)  # Использование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4a7d4",
   "metadata": {
    "id": "eac4a7d4"
   },
   "source": [
    "#### Softmax\n",
    "Стратегия Softmax представляет собой метод, который преобразует оценки ценности действий в вероятности их выбора. Эта стратегия учитывает \"мягкость\" выбора, регулируемую параметром температуры. При высокой температуре вероятности всех действий приближаются друг к другу, что способствует более случайному выбору (исследование), в то время как при низкой температуре выбирается действие с наибольшей оценкой (использованию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230741c",
   "metadata": {
    "id": "d230741c"
   },
   "outputs": [],
   "source": [
    "def softmax(Q_values, temperature):\n",
    "    exp_values = np.exp((Q_values - np.max(Q_values)) / temperature)\n",
    "    probabilities = exp_values / exp_values.sum()\n",
    "    return np.random.choice(len(Q_values), p=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44f0f7",
   "metadata": {
    "id": "ae44f0f7"
   },
   "source": [
    "#### UCB1 (Upper Confidence Bound)\n",
    "Стратегия UCB1 основана на оценке верхней границы для ценности действий. Агент выбирает действие, которое имеет максимальную смешанную оценку ценности и уверенность в этой оценке. Параметр, который регулирует уровень исследования, зависит от логарифма общего числа выполненных шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa020e",
   "metadata": {
    "id": "85aa020e"
   },
   "outputs": [],
   "source": [
    "def ucb1(Q_values, counts, total_counts):\n",
    "    ucb_values = Q_values + np.sqrt((2 * np.log(total_counts)) / counts)\n",
    "    return np.argmax(ucb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cbeec",
   "metadata": {
    "id": "d64cbeec"
   },
   "source": [
    "Каждая из этих стратегий нацелена обеспечивать баланс между исследованием (поиск новых стратегий) и эксплуатацией (использование текущих знаний) в процессе обучения с подкреплением. Выбор конкретной стратегии зависит от характеристик задачи и предпочтений, таких как уровень исследования, требуемый для успешного обучения агента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744ee93",
   "metadata": {
    "id": "f744ee93"
   },
   "source": [
    "### Тестирование различных стратегий\n",
    "На этом шаге мы рассмотрим модели реализующие некоторые из этих стратегий"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c9caf",
   "metadata": {
    "id": "045c9caf"
   },
   "source": [
    "#### DDPG\n",
    "[DDPG](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html) использует epsilon-greedy стратегию, добавляя шум к выбранному действию во время исследования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nl8PZ8JuVjn-",
   "metadata": {
    "id": "Nl8PZ8JuVjn-"
   },
   "outputs": [],
   "source": [
    "ddpg_timestamps = 10000\n",
    "\n",
    "best_params_ddpg = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"gradient_steps\": 1,\n",
    "    \"train_freq\": 1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 256,\n",
    "    \"policy_kwargs\": dict(net_arch=[400, 300])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9992a56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9992a56",
    "outputId": "63630c24-9746-47a8-a91d-8e618f82ee8c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.5 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(env=env, action_noise=action_noise, verbose=1, **best_params_ddpg)\n",
    "model.learn(total_timesteps=ddpg_timestamps, log_interval=10)\n",
    "model.save(\"ddpg_MountainCarContinuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BQOn7f372rvD",
   "metadata": {
    "id": "BQOn7f372rvD"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04274e97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 840
    },
    "id": "04274e97",
    "outputId": "12253315-4f9b-42a6-b0d7-ed47d7e099a0"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from pyvirtualdisplay import Display\n",
    "from matplotlib import animation\n",
    "\n",
    "timestamp = 400\n",
    "\n",
    "model = DDPG.load(\"ddpg_MountainCarContinuous\")\n",
    "ddpg_reward = []\n",
    "\n",
    "d = Display()\n",
    "d.start()\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "img = []\n",
    "for i in range(timestamp):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    ddpg_reward.append(reward)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    img.append(env.render())\n",
    "\n",
    "    if d:\n",
    "        env.reset()\n",
    "\n",
    "dpi = 72\n",
    "interval = 1 # ms\n",
    "\n",
    "plt.figure(figsize=(img[0].shape[1]/dpi,img[0].shape[0]/dpi),dpi=dpi)\n",
    "patch = plt.imshow(img[0])\n",
    "plt.axis=('off')\n",
    "animate = lambda i: patch.set_data(img[i])\n",
    "ani = animation.FuncAnimation(plt.gcf(),animate,frames=len(img),interval=interval)\n",
    "display.display(display.HTML(ani.to_jshtml()))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c50ce",
   "metadata": {
    "id": "0e2c50ce"
   },
   "source": [
    "#### SAC\n",
    "Стратегия исследования softmax применяется через термин энтропии в [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html). Термин энтропия поощряет мягкое, вероятностное распределение действий, способствуя исследованию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x-XEHfkJ8hSH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-XEHfkJ8hSH",
    "outputId": "77616311-da88-4be8-d4d1-97bc95b1f99f"
   },
   "outputs": [],
   "source": [
    "sac_timestamps = 50000\n",
    "\n",
    "best_params_sac = {\n",
    "    'batch_size': 512,\n",
    "    'buffer_size': 50000,\n",
    "    'ent_coef': 0.1,\n",
    "    'gamma': 0.9999,\n",
    "    'gradient_steps': 32,\n",
    "    'learning_rate': 0.0003,\n",
    "    'learning_starts': 0,\n",
    "    'policy': 'MlpPolicy',\n",
    "    'policy_kwargs': dict(log_std_init=-3.67, net_arch=[64, 64]),\n",
    "    'tau': 0.01,\n",
    "    'train_freq': 32,\n",
    "    'use_sde': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b4f7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "793b4f7c",
    "outputId": "63acfee8-a6de-4b87-95b9-2d4e01810273",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создание окружения MountainCarContinuous-v0\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode=\"rgb_array\")\n",
    "model = SAC(env=env, verbose=1, **best_params_sac)\n",
    "model.learn(total_timesteps=sac_timestamps, log_interval=10)\n",
    "model.save(\"sac_MountainCarContinuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OAI2XcAt7kPY",
   "metadata": {
    "id": "OAI2XcAt7kPY"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49880b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 840
    },
    "id": "f49880b1",
    "outputId": "52e1105d-a6ac-435d-c477-20e851e50456",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SAC.load(\"sac_MountainCarContinuous\")\n",
    "sac_reward = []\n",
    "\n",
    "d = Display()\n",
    "d.start()\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode=\"rgb_array\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "\n",
    "img = []\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    sac_reward.append(reward)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    img.append(env.render())\n",
    "\n",
    "dpi = 72\n",
    "interval = 1 # ms\n",
    "\n",
    "plt.figure(figsize=(img[0].shape[1]/dpi,img[0].shape[0]/dpi),dpi=dpi)\n",
    "patch = plt.imshow(img[0])\n",
    "plt.axis=('off')\n",
    "animate = lambda i: patch.set_data(img[i])\n",
    "ani = animation.FuncAnimation(plt.gcf(),animate,frames=len(img),interval=interval)\n",
    "display.display(display.HTML(ani.to_jshtml()))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815913b",
   "metadata": {
    "id": "6815913b"
   },
   "source": [
    "Примеры выше демонстрируют возможность решить одну и ту же задачу с помощью двух моделей различных моделей, которые используют расмотренные ранее стратегии. Далее построим график, который отображает процесс обучения этих моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adfb82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "c9adfb82",
    "outputId": "bdaaab4b-38eb-4af1-db84-041858e48ef5"
   },
   "outputs": [],
   "source": [
    "def plot_training_results(model_logs, model_names, total_steps=100):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, logs in enumerate(model_logs):\n",
    "        plt.plot(np.arange(0, total_steps, 1), logs[\"reward\"][:total_steps], label=model_names[i])\n",
    "\n",
    "    plt.title(\"Comparison\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "ddpg_logs = {\"reward\": []}\n",
    "sac_logs = {\"reward\": []}\n",
    "\n",
    "ddpg_logs[\"reward\"] = ddpg_reward\n",
    "\n",
    "sac_logs[\"reward\"] = sac_reward\n",
    "\n",
    "plot_training_results([ddpg_logs, sac_logs], [\"DDPG\", \"SAC\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767990d",
   "metadata": {
    "id": "0767990d"
   },
   "source": [
    "\n",
    "### Исследование влияния Learning Rate\n",
    "\n",
    "Learning Rate (LR) является одним из ключевых гиперпараметров алгоритмов обучения с подкреплением, включая алгоритм Soft Actor-Critic (SAC). Этот параметр оказывает значительное влияние на эффективность обучения и способность алгоритма адаптироваться к изменениям в окружающей среде.\n",
    "\n",
    "Learning Rate определяет размер шага, с которым модель обновляет свои веса в процессе градиентного спуска. В случае SAC, который является алгоритмом глубокого обучения, правильный выбор Learning Rate может определить успешность сходимости модели и её способность обучаться оптимальной стратегии.\n",
    "\n",
    "В данном блоке мы рассмотрим важность параметра Learning Rate для алгоритма SAC, исследуем, как различные значения этого параметра могут влиять на процесс обучения. Далее мы представим код, который демонстрирует обучение модели SAC при различных значениях Learning Rate, а также проанализируем графики, позволяющие визуально оценить влияние этого параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2a2ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02f2a2ac",
    "outputId": "611da7be-5413-4735-ecab-6ee9cc76ec6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создание среды MountainCarContinuous-v0 из Gym\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# Список learning rate, которые вы хотите проверить\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "sac_reward = {0.0001:[],0.001:[],0.01:[]}\n",
    "\n",
    "best_params_sac_v2 = {\n",
    "    'batch_size': 512,\n",
    "    'buffer_size': 50000,\n",
    "    'ent_coef': 0.1,\n",
    "    'gamma': 0.9999,\n",
    "    'gradient_steps': 32,\n",
    "    'learning_starts': 0,\n",
    "    'policy': 'MlpPolicy',\n",
    "    'policy_kwargs': dict(log_std_init=-3.67, net_arch=[64, 64]),\n",
    "    'tau': 0.01,\n",
    "    'train_freq': 32,\n",
    "    'use_sde': True\n",
    "}\n",
    "# Итерация по разным learning rates\n",
    "for lr in learning_rates:\n",
    "    # Создание модели SAC с заданным learning rate\n",
    "    model = SAC(env=env, learning_rate=lr, verbose=1, **best_params_sac_v2)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.learn(total_timesteps=10000)\n",
    "\n",
    "    # Сохранение обученной модели\n",
    "    model.save(f'sac_MountainCarContinuous_v0_lr_{lr}')\n",
    "    # model = SAC.load(f'sac_MountainCarContinuous_lr_{lr}')\n",
    "    obs, info = env.reset()\n",
    "    for i in range(500):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        sac_reward[lr].append(reward)\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "# Закрытие среды\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98c00f",
   "metadata": {
    "id": "2c98c00f"
   },
   "source": [
    "Проанализировав логи обучения модели можно увидеть, что выбор параметра learning_rate влияет, например, на время обучения модели.\n",
    "Так же можно посчитать средний reward для различных значений learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e3424",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be6e3424",
    "outputId": "d56dd717-b035-4944-a9b0-08f521fe9e70"
   },
   "outputs": [],
   "source": [
    "print(sum(sac_reward[0.0001])/len(sac_reward[0.0001]))\n",
    "print(sum(sac_reward[0.001])/len(sac_reward[0.001]))\n",
    "print(sum(sac_reward[0.01])/len(sac_reward[0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672a411",
   "metadata": {
    "id": "2672a411"
   },
   "source": [
    "Представим результаты reward для моделей обученных с различным learning_rate на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572403df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "572403df",
    "outputId": "3ef10a76-5415-479b-8b8c-2ec8b7d4dd10"
   },
   "outputs": [],
   "source": [
    "sac_e4,sac_e3,sac_e2 = {}, {}, {}\n",
    "\n",
    "sac_e4[\"reward\"] = sac_reward[0.0001]\n",
    "\n",
    "sac_e3[\"reward\"] = sac_reward[0.001]\n",
    "\n",
    "sac_e2[\"reward\"] = sac_reward[0.01]\n",
    "plot_training_results([sac_e4, sac_e3, sac_e2], [\"0,0001\", \"0,001\", \"0,01\"], total_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KJOpoWQZKyxj",
   "metadata": {
    "id": "KJOpoWQZKyxj"
   },
   "source": [
    "Изменение параметра learning_rate при обучении алгоритма может существенно влиять на его производительность.\n",
    "\n",
    "#### Маленький learning rate:\n",
    "\n",
    "Обучение будет медленным, так как обновления весов модели будут небольшими.\n",
    "Может потребоваться больше времени для достижения сходимости.\n",
    "Существует риск застревания в локальных минимумах.\n",
    "\n",
    "#### Средний learning rate:\n",
    "\n",
    "Может оказаться хорошим компромиссом между скоростью обучения и стабильностью.\n",
    "Быстрее, чем маленький learning_rate, но может все еще требовать достаточно большое количество времени для сходимости. Требует аккуратного подбора значения.\n",
    "\n",
    "#### Большой learning rate:\n",
    "Обучение в среднем будет быстрее, так как веса модели обновляются с большими шагами.\n",
    "Может привести к нестабильному обучению, особенно если learning rate слишком велик.\n",
    "Существует риск \"перепрыгивания\" оптимальных значений, что может замедлить или прервать сходимость.\n",
    "\n",
    "#### Рекомендации\n",
    "Начинать с относительно небольшого learning rate и постепенно увеличивать его, наблюдая за производительностью алгоритма. Можно также попробовать методы настройки гиперпараметров, чтобы автоматически подобрать оптимальные значения learning rate.\n",
    "\n",
    "Помните, что результаты могут зависеть от конкретной задачи и структуры среды, поэтому рекомендуется проводить несколько экспериментов и внимательно анализировать результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3411b20",
   "metadata": {
    "id": "d3411b20"
   },
   "source": [
    "В данной лабораторной работе мы исследовали различные стратегии и влияние learning_rate на процесс обучения агента. Эксперименты позволяют нам понять, что различные стратегии и параметры лучше подходят для разных задач.\n",
    "В качестве самостоятельной работы вы может дополнительно провести свои эксперименты, изменяя параметры и используя различные стратегии, чтобы более глубоко понять их влияние на обучение модели. Например, попробовать использовать одну из используемых в данной работе моделей на другом окружении Gym."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
