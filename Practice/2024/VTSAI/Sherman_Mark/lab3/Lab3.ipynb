{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc68cd5",
   "metadata": {
    "id": "8dc68cd5"
   },
   "source": [
    "## Discount factor\n",
    "**Discount factor**  - это параметр, используемый в теории управления и обучении с подкреплением для оценки стоимости будущих вознаграждений. Он обозначается символом $\\gamma$ (гамма).\n",
    "\n",
    "В контексте задачи обучения с подкреплением, такой как у **DQN** алгоритма, фактор дисконтирования применяется к будущим вознаграждениям, чтобы учесть уменьшение их значения с течением времени. Фактор дисконтирования позволяет агенту принимать во внимание не только мгновенные вознаграждения, но и будущие вознаграждения, приводя к более долгосрочной стратегии.\n",
    "\n",
    "**Формула для дисконтирования будущих вознаграждений** выглядит следующим образом:\n",
    "\n",
    " $G_t$ = $R_{t+1}$ + $\\gamma$ $R_{t+2}$ + $\\gamma$^2 $R_{t+3}$ + $\\ldots$ = $\\sum_{k=0}^{\\infty}$ $\\gamma$^k $R_{t+k+1}$\n",
    "\n",
    "где:\n",
    "- $G_t$ - дисконтированная сумма вознаграждений (вознаграждение с учетом будущих шагов);\n",
    "- $R_{t+k+1}$ - вознаграждение, полученное на шаге $t+k+1$;\n",
    "- $\\gamma$ - фактор дисконтирования,  $0 \\leq \\gamma \\leq 1$.\n",
    "\n",
    "Фактор дисконтирования имеет важное значение при принятии решений в условиях неопределенности, где агенту нужно учитывать как мгновенные, так и будущие вознаграждения, с учетом их временного удаления. Выбор подходящего значения для фактора дисконтирования может влиять на стратегии обучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pgaFe0w08Gg2",
   "metadata": {
    "id": "pgaFe0w08Gg2"
   },
   "source": [
    "## Long-term и Short-term стратегия\n",
    "\n",
    "Фактор дисконтирования оказывает сильное влияние на долгосрочность стратегии.\n",
    "В контексте обучения с подкреплением и стратегий агента, термины \"краткосрочные\" и \"долгосрочные\" относятся к временным характеристикам принятия решений агентом.\n",
    "\n",
    "**Краткосрочные стратегии**:\n",
    "\n",
    "> Агент, придерживающийся краткосрочной стратегии, ориентируется в основном на текущую информацию и мгновенные вознаграждения.\n",
    "Такой агент может принимать решения, которые приносят максимальное мгновенное вознаграждение, не уделяя большого внимания будущим шагам. Такой агент может принимать решения, которые приносят максимальное мгновенное вознаграждение, не уделяя большого внимания будущим шагам.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Долгосрочные стратегии**:\n",
    "\n",
    "\n",
    "> Агент, придерживающийся долгосрочной стратегии, учитывает будущие вознаграждения и последствия своих действий на протяжении более длительного времени.\n",
    "Такой агент может предпочесть действия, которые могут не приносить максимальное мгновенное вознаграждение, но могут способствовать достижению более высоких наград в будущем.\n",
    "\n",
    "Когда говорят о краткосрочных или долгосрочных стратегиях в контексте обучения с подкреплением, обычно имеется в виду, как агент принимает решения, оптимизируя сумму будущих вознаграждений. Фактор дисконтирования γ в уравнении обучения с подкреплением регулирует степень учета будущих вознаграждений, и, таким образом, влияет на то, насколько агент ориентирован на краткосрочные или долгосрочные перспективы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ZIN2MIo1pJ5",
   "metadata": {
    "id": "4ZIN2MIo1pJ5"
   },
   "source": [
    "## Метрики для оценки модели\n",
    "В данной работе мы рассмотрим некоторые метрики, которые могут быть использованы для оценки обучения модели.\n",
    "Аналогично первой лабораторной работы мы будем решать задачу в окружении LunarLander-v2, а в качестве модели используем уже знакомый DQN. При этом мы обучим модель несколько разных с разным параметром Discount factor.\n",
    "\n",
    "Для оценки модели мы рассмотрим следущие метрики:\n",
    "\n",
    "*   Q-values, их поведение и сходимость.\n",
    "*   Средняя награда за эпизод.\n",
    "*   Время затрачиваемое на обучение модели.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221acc63",
   "metadata": {
    "id": "221acc63"
   },
   "source": [
    "### Установка зависимостей\n",
    "\n",
    "На первом шаге мы начинаем с установки необходимых библиотек. Пакет [gymnasium](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) предоставляет различные среды для обучения с подкреплением, в то время как [stable-baselines3](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) предоставляет реализации различных алгоритмов обучения с подкреплением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "!apt install -y swig\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install stable-baselines3\n",
    "!pip install PyVirtualDisplay\n",
    "!sudo apt-get install xvfb\n",
    "\n",
    "!pip install 'shimmy>=0.2.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z1Fbb4br3TK2",
   "metadata": {
    "id": "Z1Fbb4br3TK2"
   },
   "source": [
    "Импортируем пакет, которые будут использованы в данной работе\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IWD3A_wi5qHh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ctnQDvYs3Xpe",
   "metadata": {
    "id": "ctnQDvYs3Xpe"
   },
   "source": [
    "Q-значения (Q-values) представляют собой оценки или оценочные функции, используемые в контексте обучения с подкреплением для измерения ожидаемой полезности (ценности) принятия определенного действия в конкретном состоянии.\n",
    "\n",
    "Для получения значений для нашей модели реализуем функцию, которая их извлекает из q-сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JdVK2QGXDcH3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
    "  # Доступ к Q-network\n",
    "  q_net = model.q_net\n",
    "\n",
    "  # Конвертируем observation в PyTorch tensor\n",
    "  device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "  obs_tensor = th.tensor(obs, dtype=th.float32).to(device)\n",
    "\n",
    "  # Изменяем размерность\n",
    "  obs_tensor = obs_tensor.unsqueeze(0)\n",
    "\n",
    "  #Извлекаем Q-values\n",
    "  q_values = model.q_net.forward(obs_tensor)\n",
    "\n",
    "  return q_values.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BDriZ7ML4X2H",
   "metadata": {
    "id": "BDriZ7ML4X2H"
   },
   "source": [
    "Для наглядности результатов Q-values необходимо создать функцию для составления графика по значениям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ZIyURWpg8h",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values(q_values_list):\n",
    "  # Извлечение отдельных списков для каждого Q-значения\n",
    "  q1_values, q2_values, q3_values, q4_values = zip(*q_values_list)\n",
    "\n",
    "  # Построение графика\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(q1_values, label='Q1 Values')\n",
    "  plt.plot(q2_values, label='Q2 Values')\n",
    "  plt.plot(q3_values, label='Q3 Values')\n",
    "  plt.plot(q4_values, label='Q4 Values')\n",
    "  plt.xlabel('Time')\n",
    "  plt.ylabel('Q-Values')\n",
    "  plt.title('Convergence of Q-Values over Time')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kdwvvV754rHo",
   "metadata": {
    "id": "kdwvvV754rHo"
   },
   "source": [
    "Следующий блок кода служит для следующих целей:\n",
    "\n",
    "*   Создание окружения\n",
    "*   Создание модели\n",
    "*   Обучение модели\n",
    "*   Оценка модели до обучения\n",
    "*   Оценка модели после обучения\n",
    "*   Измерение времени обучени\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uOdMyNh7QKER",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward(discount_factor, params = None):\n",
    "  #Создание окружения\n",
    "  env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "\n",
    "  #Создание модели\n",
    "  if params:\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, gamma=discount_factor, **params)\n",
    "  else:\n",
    "    model = DQN(\"MlpPolicy\", env, verbose=1, gamma=discount_factor)\n",
    "\n",
    "  #Количество эпизодов для оценки модели\n",
    "  n_eval_episodes = 250\n",
    "\n",
    "  #Оценка модели до обучения\n",
    "  mean_reward, std_reward = evaluate_policy(model, env, deterministic=True, n_eval_episodes=n_eval_episodes)\n",
    "  print(f\"До обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "  # Засекаем начальное время\n",
    "  start_time = time.time()\n",
    "\n",
    "  #Обучение модели\n",
    "  model.learn(total_timesteps=100000, log_interval=100)\n",
    "\n",
    "  # Засекаем время завершения\n",
    "  end_time = time.time()\n",
    "\n",
    "  #Оценка модели после обучения\n",
    "  mean_reward, std_reward = evaluate_policy(model, env, deterministic=True, n_eval_episodes=n_eval_episodes)\n",
    "  print(f\"После обучения модели с discount_factor = {discount_factor}, mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "  model.save(f\"dqn_lunarlander_{discount_factor}\")\n",
    "  del model\n",
    "\n",
    "  # Вычисляем время обучений\n",
    "  learn_time = end_time - start_time\n",
    "  return env, learn_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9keB1T9Y519e",
   "metadata": {
    "id": "9keB1T9Y519e"
   },
   "source": [
    "Q-values необходимо вычислять для некоторого набора предсказаний модели, реализуем для этого функцию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utVz8HWu7pXd",
   "metadata": {
    "id": "utVz8HWu7pXd"
   },
   "source": [
    "Рассмотрим 3 значения discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JsFNBuPn-4cQ",
   "metadata": {
    "id": "JsFNBuPn-4cQ"
   },
   "source": [
    "## Модель с стандартными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3-n6IC_cxs3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_values_calculation(discount_factor, env):\n",
    "  #Загружаем созданную и созраненную модель\n",
    "  model = DQN.load(f\"dqn_lunarlander_{discount_factor}\")\n",
    "\n",
    "  #В LunarLander окружении возможны только 4 действия\n",
    "  # 0: do nothing\n",
    "  # 1: fire left orientation engine\n",
    "  # 2: fire main engine\n",
    "  # 3: fire right orientation engine\n",
    "\n",
    "  action_str = ['nothing', 'left', 'main', 'right']\n",
    "  q_values_list = []\n",
    "\n",
    "  obs, info = env.reset()\n",
    "  for _ in range(500):\n",
    "      q_val = calc_q_values(model,obs)\n",
    "      q_values_list.append(q_val)\n",
    "      action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "      print(f\"Q-value состояния nothing={q_val[0]:.2f} left={q_val[1]:.2f} main={q_val[1]:.2f} right={q_val[1]:.2f}\")\n",
    "      print(f\"Действие: {action_str[action]}\")\n",
    "\n",
    "      obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  return q_values_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j37Byubd6fEI",
   "metadata": {
    "id": "j37Byubd6fEI"
   },
   "source": [
    "Изменение значения discount factor (γ) в DQN может оказать существенное воздействие на обучение агента и его стратегию.\n",
    "\n",
    "Воздействие на краткосрочные или долгосрочные стратегии:\n",
    "\n",
    "*   При более высоком значении γ (ближе к 1) агент будет больше учитывать будущие вознаграждения, что может способствовать формированию более долгосрочных стратегий.\n",
    "\n",
    "*  При более низком значении γ (ближе к 0) агент будет более склонен к учету только мгновенных вознаграждений, что может привести к формированию краткосрочных стратегий.\n",
    "\n",
    "\n",
    "Влияние на стабильность обучения:\n",
    "\n",
    "*   Высокий γ может сделать обучение более стабильным, особенно в средах с большим количеством шума или случайности.\n",
    "*   Низкий γ может привести к более нестабильному обучению, так как агент будет менее чувствителен к долгосрочным последствиям своих действий.\n",
    "\n",
    "\n",
    "Влияние на временные рамки обучения:\n",
    "\n",
    "\n",
    "*   Различные значения γ могут потребовать различных временных рамок обучения для достижения оптимальных стратегий.\n",
    "*   Изменение γ может потребовать перенастройки гиперпараметров и количества шагов обучения.\n",
    "\n",
    "\n",
    "Адаптивность к задаче:\n",
    "*  Эффективное значение γ может зависеть от конкретной задачи. Например, в задачах с большой неопределенностью и случайностью, более высокий γ может быть полезен.\n",
    "\n",
    "\n",
    "Экспериментальные результаты:\n",
    "*  Чтобы определить оптимальное значение γ для вашей конкретной задачи, часто требуется провести эксперименты с различными значениями, а затем анализировать результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LLtYL9vE9glW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tLOgee1-9h28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.spec.max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Me6HrWLOvPSV",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factors = [0.01,0.5,0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bljIvKmNVY",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment, time_to_lrn = mean_reward(discount_factors[0])\n",
    "q_vals = q_values_calculation(discount_factors[0], environment)\n",
    "plot_q_values(q_vals)\n",
    "print(f'Время обучения модели при discount_factor = {discount_factors[0]} : {time_to_lrn} секунд.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c178e6",
   "metadata": {
    "id": "57c178e6"
   },
   "source": [
    "#### Закрытие среды после тестирования\n",
    "По завершении тестирования мы закрываем среду с помощью env.close()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912717a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрытие среды\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11d65b",
   "metadata": {
    "id": "aa11d65b"
   },
   "source": [
    "В рамках самостоятельной работы попробуйте обучить и протестировать модель на другой среды доступной в gymnasium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uGqXV8xwuuYG",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment, time_to_lrn = mean_reward(discount_factors[1])\n",
    "q_vals = q_values_calculation(discount_factors[1], environment)\n",
    "plot_q_values(q_vals)\n",
    "print(f'Время обучения модели при discount_factor = {discount_factors[1]} : {time_to_lrn} секунд.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0iZn9Uywy666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрытие среды\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XeqFpyzbyvOl",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment, time_to_lrn = mean_reward(discount_factors[2])\n",
    "q_vals = q_values_calculation(discount_factors[2], environment)\n",
    "plot_q_values(q_vals)\n",
    "print(f'Время обучения модели при discount_factor = {discount_factors[2]} : {time_to_lrn} секунд.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6uQqaFsiy9ai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрытие среды\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "log95qE3-zO6",
   "metadata": {
    "id": "log95qE3-zO6"
   },
   "source": [
    "## Модель с оптимальными параметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9MzqpBf-eAe",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 128,\n",
    "    'buffer_size': 50000,\n",
    "    'exploration_final_eps': 0.1,\n",
    "    'exploration_fraction': 0.12,\n",
    "    'gradient_steps': -1,\n",
    "    'learning_rate': 0.00063,\n",
    "    'learning_starts': 0,\n",
    "    'policy_kwargs': dict(net_arch=[256, 256]),\n",
    "    'target_update_interval': 250,\n",
    "    'train_freq': 4\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3BEbFSIE-d2h",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment, time_to_lrn = mean_reward(discount_factors[0], params)\n",
    "q_vals = q_values_calculation(discount_factors[0], environment)\n",
    "plot_q_values(q_vals)\n",
    "print(f'Время обучения модели при discount_factor = {discount_factors[0]} : {time_to_lrn} секунд.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoQWzLOD_K2S",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрытие среды\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LcwSWMcT_Lan",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment, time_to_lrn = mean_reward(discount_factors[1], params)\n",
    "q_vals = q_values_calculation(discount_factors[1], environment)\n",
    "plot_q_values(q_vals)\n",
    "print(f'Время обучения модели при discount_factor = {discount_factors[1]} : {time_to_lrn} секунд.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TAZSWCbh_Oog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрытие среды\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nzTZpncd_TLt",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment, time_to_lrn = mean_reward(discount_factors[2], params)\n",
    "q_vals = q_values_calculation(discount_factors[2], environment)\n",
    "plot_q_values(q_vals)\n",
    "print(f'Время обучения модели при discount_factor = {discount_factors[2]} : {time_to_lrn} секунд.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8MhCMYnG_UYL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрытие среды\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DRO6Jtru73mo",
   "metadata": {
    "id": "DRO6Jtru73mo"
   },
   "source": [
    "Мы завершили лабораторную работу, посвященную изучению дисконтного фактора и ряда метрик, связанных с обучением с подкреплением на примере алгоритма DQN.\n",
    "\n",
    "Для того чтобы закрепить полученные знания, предлагается подобрать наиболее оптимальный параметр для рассмторенной задачи или самостоятельно реализовать решение другой задачи, настраивая дисконтный фактор и анализируя изученные метрики. Это позволит лучше понять влияние дисконтного фактора на обучение агента в различных контекстах и освоить навыки тюнинга параметров для достижения оптимальных результатов.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
