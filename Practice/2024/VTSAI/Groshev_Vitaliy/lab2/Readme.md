# Лабораторная работа №2
### Эксперименты с обучением с подкреплением в среде MountainCarContinuous-v0
Выполнил: Виталий Грошев
#### Описание
Цель этих экспериментов - подобрать оптимальный коэффициент обучения
(Learning Rate, lr) для стратегии обучения с подкреплением.
Было проведено тестирование Soft Actor-Critic (SAC) и DDPG с различными 
значениеми коэффициента обучения (0.0001, 0.001, 0.01, 0.0004, 0.004, 0.04) 
с использованием стратегий softmax b и Epsilon-Greedy соответсвенно.


#### Результаты Экспериментов
Первоначальный эксперимент с SAC

При коэффициенте обучения 0.02
Средняя награда: -1.739-06
Не удалось достичь сходимость, отсутсвие колебаний. 

При коэффициенте обучения 0.002
Средняя награда: -4.4611e-07
Этот коэффициент обучения вызвал колебания в процессе обучения и долгую сходимость.

При коэффициенте обучения 0.0002
Средняя награда: -1.739e-06
Этот коэффициент обучения вызвал колебания в процессе обучения и долгую сходимость.

После первоначального эксперимента SAC

Следующие эксперименты проводились со значениями lr: 0.0003, 0.005, 0.00005

При коэффициенте обучения 0.0003
Средняя награда: -4.809e-05

При коэффициенте обучения 0.005
Средняя награда: -3.45-e06

При коэффициенте обучения 0.00005
Средняя награда: -2.81-e06

Для всех эскпериментов количество шагов было установлено в 500000. 
И ни при одном значении коэффициента lr не удалось достичь сходимости при обучении с данным количеством шагов.

#### Заключение
Проведенные эксперименты показывают, что для поиска правильного коэффициента обучения для данной среды требуется много проб и ошибок.
Возможно автоматический перебор гиперпараметров был бы здесь полезен.
Так же, возможно что стратегии SAC и DDPG малоэфффективны именно для выбранной среды MountainCarContinuous-v0.