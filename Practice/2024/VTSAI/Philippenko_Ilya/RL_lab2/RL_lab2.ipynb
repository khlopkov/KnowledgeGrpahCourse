{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc68cd5",
   "metadata": {
    "id": "8dc68cd5"
   },
   "source": [
    "## Стратегии обучения RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221acc63",
   "metadata": {
    "id": "221acc63"
   },
   "source": [
    "### Подготовка среды и библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DPoVvCSGaS4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyvirtualdisplay swig\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC,DDPG\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b153d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание окружения\n",
    "env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2713d",
   "metadata": {
    "id": "c8e2713d"
   },
   "source": [
    "### Исследование различных стратегий\n",
    "Исследуем влияние различных стратегий исследования на процесс обучения агента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677e9b6",
   "metadata": {
    "id": "8677e9b6"
   },
   "source": [
    "#### Epsilon-Greedy\n",
    "\n",
    "Epsilon-Greedy - это одна из базовых стратегий исследования в обучении с подкреплением. Агент принимает решение о выборе действия с учетом двух возможных вариантов: с высокой вероятностью (1-epsilon) агент выбирает действие с максимальной оценкой (использование), а с низкой вероятностью (epsilon) - случайное действие (исследование). Это позволяет агенту совмещать использование текущих знаний с возможностью исследования новых стратегий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d2bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(len(Q_values))  # Исследование\n",
    "    else:\n",
    "        return np.argmax(Q_values)  # Использование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4a7d4",
   "metadata": {
    "id": "eac4a7d4"
   },
   "source": [
    "#### Softmax\n",
    "Стратегия Softmax представляет собой метод, который преобразует оценки ценности действий в вероятности их выбора. Эта стратегия учитывает \"мягкость\" выбора, регулируемую параметром температуры. При высокой температуре вероятности всех действий приближаются друг к другу, что способствует более случайному выбору (исследование), в то время как при низкой температуре выбирается действие с наибольшей оценкой (использованию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Q_values, temperature):\n",
    "    exp_values = np.exp((Q_values - np.max(Q_values)) / temperature)\n",
    "    probabilities = exp_values / exp_values.sum()\n",
    "    return np.random.choice(len(Q_values), p=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae44f0f7",
   "metadata": {
    "id": "ae44f0f7"
   },
   "source": [
    "#### UCB1 (Upper Confidence Bound)\n",
    "Стратегия UCB1 основана на оценке верхней границы для ценности действий. Агент выбирает действие, которое имеет максимальную смешанную оценку ценности и уверенность в этой оценке. Параметр, который регулирует уровень исследования, зависит от логарифма общего числа выполненных шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb1(Q_values, counts, total_counts):\n",
    "    ucb_values = Q_values + np.sqrt((2 * np.log(total_counts)) / counts)\n",
    "    return np.argmax(ucb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cbeec",
   "metadata": {
    "id": "d64cbeec"
   },
   "source": [
    "Каждая из этих стратегий нацелена обеспечивать баланс между исследованием (поиск новых стратегий) и эксплуатацией (использование текущих знаний) в процессе обучения с подкреплением. Выбор конкретной стратегии зависит от характеристик задачи и предпочтений, таких как уровень исследования, требуемый для успешного обучения агента."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744ee93",
   "metadata": {
    "id": "f744ee93"
   },
   "source": [
    "### Тестирование различных стратегий\n",
    "На этом шаге мы рассмотрим модели реализующие некоторые из этих стратегий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NOjBSkslvtmx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# глобальные параметры, равны для сравнения моделей\n",
    "timestamp = 50\n",
    "total = 500\n",
    "freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c9caf",
   "metadata": {
    "id": "045c9caf"
   },
   "source": [
    "#### DDPG\n",
    "[DDPG](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html) использует epsilon-greedy стратегию, добавляя шум к выбранному действию во время исследования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9992a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1, train_freq=freq)\n",
    "model.learn(total_timesteps=total, log_interval=1)\n",
    "model.save(\"ddpg_pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04274e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "from pyvirtualdisplay import Display\n",
    "from matplotlib import animation\n",
    "\n",
    "del model\n",
    "\n",
    "\n",
    "model = DDPG.load(\"ddpg_pendulum\")\n",
    "ddpg_reward = []\n",
    "\n",
    "d = Display()\n",
    "d.start()\n",
    "\n",
    "env = gym.make('CarRacing-v2')\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "img = []\n",
    "for i in range(timestamp):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated = env.step(action)\n",
    "    ddpg_reward.append(reward)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    img.append(env.render('rgb_array'))\n",
    "\n",
    "    if terminated or truncated:\n",
    "        env.reset()\n",
    "\n",
    "dpi = 72\n",
    "interval = 1 # ms\n",
    "\n",
    "plt.figure(figsize=(img[0].shape[1]/dpi,img[0].shape[0]/dpi),dpi=dpi)\n",
    "patch = plt.imshow(img[0])\n",
    "plt.axis=('off')\n",
    "animate = lambda i: patch.set_data(img[i])\n",
    "ani = animation.FuncAnimation(plt.gcf(),animate,frames=len(img),interval=interval)\n",
    "display.display(display.HTML(ani.to_jshtml()))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c50ce",
   "metadata": {
    "id": "0e2c50ce"
   },
   "source": [
    "#### SAC\n",
    "Стратегия исследования softmax применяется через термин энтропии в [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html). Термин энтропия поощряет мягкое, вероятностное распределение действий, способствуя исследованию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# Создание окружения CarRacing-v2\n",
    "env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, train_freq=freq)\n",
    "model.learn(total_timesteps=total, log_interval=1)\n",
    "model.save(\"sac_pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49880b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "del model\n",
    "\n",
    "model = SAC.load(\"sac_pendulum\")\n",
    "sac_reward = []\n",
    "\n",
    "d = Display()\n",
    "d.start()\n",
    "\n",
    "env = gym.make('CarRacing-v2')\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "img = []\n",
    "for i in range(timestamp):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated = env.step(action)\n",
    "    sac_reward.append(reward)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    img.append(env.render('rgb_array'))\n",
    "\n",
    "    if terminated or truncated:\n",
    "        env.reset()\n",
    "\n",
    "dpi = 72\n",
    "interval = 1 # ms\n",
    "\n",
    "plt.figure(figsize=(img[0].shape[1]/dpi,img[0].shape[0]/dpi),dpi=dpi)\n",
    "patch = plt.imshow(img[0])\n",
    "plt.axis=('off')\n",
    "animate = lambda i: patch.set_data(img[i])\n",
    "ani = animation.FuncAnimation(plt.gcf(),animate,frames=len(img),interval=interval)\n",
    "display.display(display.HTML(ani.to_jshtml()))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815913b",
   "metadata": {
    "id": "6815913b"
   },
   "source": [
    "Примеры выше демонстрируют возможность решить одну и ту же задачу с помощью двух моделей различных моделей, которые используют расмотренные ранее стратегии. Далее построим график, который отображает процесс обучения этих моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adfb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(model_logs, model_names, total_steps=50):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, logs in enumerate(model_logs):\n",
    "        plt.plot(np.arange(0, total_steps, 1), logs[\"reward\"][:total_steps], label=model_names[i])\n",
    "\n",
    "    plt.title(\"Comparison: SAC vs DDPG\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "ddpg_logs = {\"reward\": []}\n",
    "sac_logs = {\"reward\": []}\n",
    "\n",
    "ddpg_logs[\"reward\"] = ddpg_reward\n",
    "\n",
    "sac_logs[\"reward\"] = sac_reward\n",
    "\n",
    "plot_training_results([ddpg_logs, sac_logs], [\"DDPG\", \"SAC\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767990d",
   "metadata": {
    "id": "0767990d"
   },
   "source": [
    "\n",
    "### Исследование влияния Learning Rate\n",
    "\n",
    "Learning Rate (LR) является одним из ключевых гиперпараметров алгоритмов обучения с подкреплением, включая алгоритм Soft Actor-Critic (SAC). Этот параметр оказывает значительное влияние на эффективность обучения и способность алгоритма адаптироваться к изменениям в окружающей среде.\n",
    "\n",
    "Learning Rate определяет размер шага, с которым модель обновляет свои веса в процессе градиентного спуска. В случае SAC, который является алгоритмом глубокого обучения, правильный выбор Learning Rate может определить успешность сходимости модели и её способность обучаться оптимальной стратегии.\n",
    "\n",
    "В данном блоке мы рассмотрим важность параметра Learning Rate для алгоритма SAC, исследуем, как различные значения этого параметра могут влиять на процесс обучения. Далее мы представим код, который демонстрирует обучение модели SAC при различных значениях Learning Rate, а также проанализируем графики, позволяющие визуально оценить влияние этого параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CarRacing-v2')\n",
    "\n",
    "# Список learning rate, которые вы хотите проверить\n",
    "learning_rates = [0.0001, 0.005, 0.02]\n",
    "sac_reward = {0.0001:[],0.005:[],0.02:[]}\n",
    "\n",
    "# Итерация по разным learning rates\n",
    "for lr in learning_rates:\n",
    "    # Создание модели SAC с заданным learning rate\n",
    "    model = SAC('MlpPolicy', env, learning_rate=lr, verbose=1, train_freq=3*freq)\n",
    "\n",
    "    # Обучение модели\n",
    "    model.learn(total_timesteps=total)\n",
    "\n",
    "    # Сохранение обученной модели\n",
    "    model.save(f'sac_pendulum_lr_{lr}')\n",
    "    obs, info = env.reset()\n",
    "    for i in range(50):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        sac_reward[lr].append(reward)\n",
    "        if terminated or truncated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "# Закрытие среды\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98c00f",
   "metadata": {
    "id": "2c98c00f"
   },
   "source": [
    "Проанализировав логи обучения модели можно увидеть, что выбор параметра learning_rate влияет, например, на время обучения модели.\n",
    "Так же можно посчитать средний reward для различных значений learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(sac_reward[0.0001])/len(sac_reward[0.0001]))\n",
    "print(sum(sac_reward[0.005])/len(sac_reward[0.005]))\n",
    "print(sum(sac_reward[0.02])/len(sac_reward[0.02]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672a411",
   "metadata": {
    "id": "2672a411"
   },
   "source": [
    "Представим результаты reward для моделей обученных с различным learning_rate на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572403df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_e4,sac_e3,sac_e2 = {}, {}, {}\n",
    "\n",
    "sac_e4[\"reward\"] = sac_reward[0.0001]\n",
    "\n",
    "sac_e3[\"reward\"] = sac_reward[0.005]\n",
    "\n",
    "sac_e2[\"reward\"] = sac_reward[0.02]\n",
    "plot_training_results([sac_e4, sac_e3, sac_e2], [\"0,0001\", \"0,005\", \"0,02\"], total_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KJOpoWQZKyxj",
   "metadata": {
    "id": "KJOpoWQZKyxj"
   },
   "source": [
    "Изменение параметра learning_rate при обучении алгоритма может существенно влиять на его производительность.\n",
    "\n",
    "#### Маленький learning rate:\n",
    "\n",
    "Обучение будет медленным, так как обновления весов модели будут небольшими.\n",
    "Может потребоваться больше времени для достижения сходимости.\n",
    "Существует риск застревания в локальных минимумах.\n",
    "\n",
    "#### Средний learning rate:\n",
    "\n",
    "Может оказаться хорошим компромиссом между скоростью обучения и стабильностью.\n",
    "Быстрее, чем маленький learning_rate, но может все еще требовать достаточно большое количество времени для сходимости. Требует аккуратного подбора значения.\n",
    "\n",
    "#### Большой learning rate:\n",
    "Обучение в среднем будет быстрее, так как веса модели обновляются с большими шагами.\n",
    "Может привести к нестабильному обучению, особенно если learning rate слишком велик.\n",
    "Существует риск \"перепрыгивания\" оптимальных значений, что может замедлить или прервать сходимость.\n",
    "\n",
    "Эвристики, которые были вскольз упомянуты на первом курсе - значения LR в диапазоне [0.002, 0.005] - оказались оптимальными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3411b20",
   "metadata": {
    "id": "d3411b20"
   },
   "source": [
    "В данной лабораторной работе мы исследовали различные стратегии и влияние learning_rate на процесс обучения агента. Эксперименты позволяют нам понять, что различные стратегии и параметры лучше подходят для разных задач.\n",
    "\n",
    "Разница в наградах в ходе обучения между epsilon-greedy стратегией и softmax минимальна. Единственное отличие - softmax чаще получал минимальные награды, что свидетельствует о больших скачках вследствие большего приоритета в исследовании."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
