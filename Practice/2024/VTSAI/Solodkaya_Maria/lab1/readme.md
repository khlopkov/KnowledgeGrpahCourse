# Лабораторная работа 1

Выполнили:
* Полежаева Евгения (P4240)
* Солодкая Мария (P4240)

### Reinforcement Learning (RL)

Reinforcement Learning (Обучение с подкреплением) — это класс машинного обучения, в котором агент обучается принимать решения путем взаимодействия с окружающей средой. Агент принимает действия, получает обратную связь в виде вознаграждения или штрафа, и стремится максимизировать кумулятивное вознаграждение.

### Q-learning
[Q-learning](https://en.wikipedia.org/wiki/Q-learning) — это один из методов обучения с подкреплением, используемый для обучения агента принимать оптимальные действия в конкретной среде. Агент стремится выучить функцию Q, которая оценивает ожидаемую награду для каждой пары состояние-действие.

### Тестирование и Валидация

В контексте RL, тестирование и валидация играют важную роль. После обучения модели агента необходимо оценить ее производительность (тестирование) и убедиться в ее способности обобщения к различным ситуациям (валидация).

Тестирование в RL включает в себя запуск обученной модели в реальной среде и измерение ее производительности на основе определенных метрик. Валидация, с другой стороны, может включать в себя проверку способности модели адаптироваться к новым условиям или изменениям в среде.

## Выбор среды

В данной работе мы будем использовать среду [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/#frozen-lake), предоставляемую Gym. Эта задача заключается в пересечении замерзшего озера от старта до цели, не проваливаясь ни в какие ямы, проходя по замерзшему озеру. Игрок не всегда может двигаться в намеченном направлении из-за скользкого характера замерзшего озера.

### Создание среды FrozenLake

Мы используем метод `gym.make()` для создания среды с именем `'FrozenLake-v1'`. Эта среда предоставляет задачу с замерзшим озером.

**Параметры:**

* `desc=None`: Используется для указания карт без предварительной загрузки.

* `map_name="4x4"`: Идентификатор для использования любой из предварительно загруженных карт.

* `is_slippery=True`: Если значение true, игрок будет двигаться в намеченном направлении с вероятностью 1/3, остальные игроки будут двигаться в любом перпендикулярном направлении с равной вероятностью 1/3 в обоих направлениях.

Например, если действие оставлено, а значение `is_slippery` равно `True`, то:

P (движение влево) = 1/3

P (движение вверх) = 1/3

P (двигаться вниз) = 1/3

## Выводы:

1. Изучены новые библиотеки и зависимости (Gym, Stable-baselines3, PyVirtualDisplay и Xvfb). Они были использованы для создания среды и реализации алгоритма обучения с подкреплением на примере задачи FrozenLake-v1, а также визуализации результатов обучения в Colab с использованием анимации.
2. Изучен алгоритм DQN и использован для обучения модели глубокого обучения в среде FrozenLake-v1.
3. Для виртуализации окружения в Colab был использован виртуальный дисплей Xvfb, а также PyVirtualDisplay и Matplotlib, которые позволили отобразить результаты тестирования обученной модели в виде анимации.
4. Обученная модель была использована в задаче прогнозирования действий в среде FrozenLake-v1. Цикл, который это выполнял, записывал кадры среды в виде изображений, после чего была создана анимация.
